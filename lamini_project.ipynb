{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-30T03:38:39.244438Z",
     "start_time": "2024-04-30T03:38:35.695099Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T03:39:10.830208Z",
     "start_time": "2024-04-30T03:39:05.261991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#load dataset\n",
    "dataset = load_dataset(\"lamini/taylor_swift\",split=\"train\")"
   ],
   "id": "381e0adcaaaecc8c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T03:39:14.189128Z",
     "start_time": "2024-04-30T03:39:13.310356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ],
   "id": "f136e4ba7d2b1441",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertLMHeadModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T03:39:20.930730Z",
     "start_time": "2024-04-30T03:39:20.863633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get the data\n",
    "df = pd.DataFrame(dataset)"
   ],
   "id": "498d33dea3f4bee6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T04:00:37.977796Z",
     "start_time": "2024-04-30T04:00:37.690959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['question'], examples['answer'], truncation=True, padding=\"max_length\")\n",
    "tokenized_data = df.apply(lambda x: tokenize_function(x), axis=1)\n",
    "train_dataset1, val_dataset2 = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_dataset = train_dataset1.apply(lambda x: tokenize_function(x), axis=1)\n",
    "val_dataset = val_dataset2.apply(lambda x: tokenize_function(x), axis=1)\n",
    "test_dict = val_dataset2.to_dict(orient='records')\n"
   ],
   "id": "d920623f66b9405",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T04:00:47.999921Z",
     "start_time": "2024-04-30T04:00:47.949672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AdamW\n",
    "import accelerate\n",
    "\n",
    "#training parameters\n",
    "# Hyperparameters and Training Configuration\n",
    "learning_rate = 1.0e-5\n",
    "num_epochs = 3\n",
    "batch_size = 20\n",
    "# Define the optimizer\n",
    "\n",
    "outputdir = f\"swift_{num_epochs}\"\n",
    "training_args = TrainingArguments(learning_rate= learning_rate, per_device_train_batch_size= 20, num_train_epochs=num_epochs, weight_decay=0.01, warmup_ratio=0.1, optim=\"adafactor\",eval_steps=120, save_steps=120, gradient_accumulation_steps=4, output_dir= outputdir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ],
   "id": "340b935d174a16df",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T04:00:48.476244Z",
     "start_time": "2024-04-30T04:00:48.448701Z"
    }
   },
   "cell_type": "code",
   "source": "trainer = Trainer(model= model, args=training_args, train_dataset= train_dataset, eval_dataset=val_dataset)",
   "id": "5e5e1a475fe4637c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\LLM\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T04:00:48.837936Z",
     "start_time": "2024-04-30T04:00:48.824117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    \"\"\"for batch in train_loader:\n",
    "      \n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Average Loss: {total_loss / len(train_dataset)}\")\"\"\"\n"
   ],
   "id": "c23344c19b57f57f",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T04:00:49.341869Z",
     "start_time": "2024-04-30T04:00:49.337411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "\"\"\"with torch.no_grad():\n",
    "   for batch in val_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}\")\"\"\""
   ],
   "id": "88b1dcb7e9d7926b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with torch.no_grad():\\n   for batch in val_loader:\\n        input_ids = batch[\"input_ids\"].to(device)\\n        attention_mask = batch[\"attention_mask\"].to(device)\\n        labels = batch[\"label\"].to(device)\\n\\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\\n        predictions = torch.argmax(outputs.logits, dim=1)\\n\\n        total += labels.size(0)\\n        correct += (predictions == labels).sum().item()\\n\\naccuracy = correct / total\\nprint(f\"Validation Accuracy: {accuracy:.2f}\")'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T04:00:49.942839Z",
     "start_time": "2024-04-30T04:00:49.937009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "#max_input_tokens=1000, max_output_tokens=100, you can change this depending on if you want the model to generate more or less\n",
    "  # Tokenize text coming in\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "  )\n",
    "\n",
    "  # Generate based on the tokens\n",
    "  device = model.device\n",
    "  #generate based on tokens!\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "  #put model on gpu or cpu\n",
    "    input_ids=input_ids.to(device),\n",
    "    max_length=max_output_tokens\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "\n",
    "  # Strip the prompt\n",
    "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "\n",
    "  return generated_text_answer"
   ],
   "id": "64f392308bd45a0c",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T04:01:07.256485Z",
     "start_time": "2024-04-30T04:01:06.604562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_text = test_dict[0]['question']\n",
    "print(\"Question input (test):\", test_text)\n",
    "print(f\"Correct answer from Lamini docs: {test_dict[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference(test_text, model, tokenizer))"
   ],
   "id": "e6977cfb716919af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): What is the controversy surrounding Taylor Swift's Live from Paris concert and how does it affect her millennial fan base who are subscribed to her Reputation album?\n",
      "Correct answer from Lamini docs: Taylor Swift's Live from Paris Concert was a live streamed event that took place on December 14, 2018. The controversy surrounding the concert was that it was only available to fans who had purchased her Reputation album. This caused a lot of backlash from fans who felt that they were being unfairly excluded from the event. Many fans felt that the concert should have been made available to all of her fans, regardless of whether or not they had purchased the album. This controversy has had a significant impact on Taylor Swift's millennial fan base who are primarily subscribed to her Reputation album. Many fans felt that this was a way for Taylor Swift to force them to purchase her album in order to see the concert. This\n",
      "Model's answer: \n",
      ".................................................................\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T04:01:08.210051Z",
     "start_time": "2024-04-30T04:01:08.198392Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "448cdddc5e0c8e58",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8fd68f13a114db5a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
